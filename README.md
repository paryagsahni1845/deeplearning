# Deep Learning Implementations from Scratch

This repository contains from-scratch implementations of various deep learning models using PyTorch, applied on multiple datasets such as IMDB, Cipher, and others. The goal of this project is to demonstrate a strong understanding of neural network architectures, forward and backward propagation, and model training without relying on high-level abstractions.

## Implemented Models

- **Perceptron**: Basic feedforward neural network implemented from scratch.
- **CNN** (Convolutional Neural Network): Applied for image classification tasks like MNIST.
- **RNN** (Recurrent Neural Network): Implemented for sequential data tasks.
- **LSTM** (Long Short-Term Memory): Improved RNN variant for handling long-term dependencies.
- **Transformer**: Self-attention-based model for sequence modeling tasks.
- **ReLU Activation Functions**: Implemented custom activation layers to understand non-linear transformations.

## Features

- **Dataset Variety**: Models trained on datasets like MNIST and Cipher to cover both image and sequential data.
- **From Scratch in PyTorch**: No high-level libraries; manually implemented forward pass, backward pass, and optimization.
- **Hands-on Learning**: Focused on understanding the mathematical and computational aspects of deep learning.

## Skills Demonstrated

- PyTorch from-scratch implementation
- Neural network architectures (Perceptron, CNN, RNN, LSTM, Transformer)
- Forward and backward propagation understanding
- Model evaluation and visualization
- Handling image and sequential datasets
  
