{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBeZSzLLD9dLNdEQ5ulQpu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paryagsahni1845/deeplearning/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN\n",
        "An RNN (Recurrent Neural Network) is a type of neural network that processes sequences step-by-step, carrying forward a “memory” of past steps.\n",
        "Use it when data has an order or time relationship, like text, speech, or time series.\n",
        "It works well for tasks like sentiment analysis, language modeling, translation, or stock prediction.\n",
        "We use it because it can capture patterns that depend on earlier parts of the input, unlike regular feedforward networks."
      ],
      "metadata": {
        "id": "S97j7Jrjrwyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ScGev65NV_7q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\"rec.sport.baseball\", \"sci.space\"]  # just 2 categories\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Tokenize ---\n",
        "def tokenize(text):\n",
        "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "\n",
        "# --- 2. Build vocab (limit size) ---\n",
        "all_tokens = []\n",
        "for doc in newsgroups.data:\n",
        "    all_tokens.extend(tokenize(doc))\n",
        "\n",
        "max_vocab = 5000\n",
        "counter = Counter(all_tokens)\n",
        "vocab = {word: i+2 for i, (word, _) in enumerate(counter.most_common(max_vocab))}\n",
        "vocab[\"<PAD>\"] = 0\n",
        "vocab[\"<UNK>\"] = 1\n",
        "\n",
        "# --- 3. Encode documents into sequences of word IDs ---\n",
        "def encode(text):\n",
        "    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokenize(text)]\n",
        "\n",
        "X_seq = [encode(doc) for doc in newsgroups.data]\n",
        "y = newsgroups.target\n",
        "\n",
        "# --- 4. Pad / clip to fixed length ---\n",
        "max_len = 30\n",
        "X = np.array([x[:max_len] if len(x) >= max_len else x + [0]*(max_len - len(x)) for x in X_seq])\n",
        "\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "hRnIdfpkd4b2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = NewsDataset(X_train, y_train)\n",
        "test_dataset = NewsDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "_q-qwEjFd9dB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "test_dataset = TextDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "dDvdMAm7eWc-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64, num_classes=20):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = out[:, -1, :]  # take last timestep\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ctf7gDmzecu5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = RNNClassifier(len(vocab), 64, 64, len(newsgroups.target_names)).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=1e-3)"
      ],
      "metadata": {
        "id": "KRtVOulWegMF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.utils.clip_grad_norm_(model.parameters(), 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBdnL3772qMu",
        "outputId": "8d598e85-c900-462c-9abc-62f9bc0efdb9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):  # keep small for demo\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += (outputs.argmax(1) == y_batch).sum().item()\n",
        "\n",
        "    acc = total_correct / len(train_dataset)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Train Acc: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FccP_QEejgZ",
        "outputId": "3794df3c-ba69-4b9f-daae-a5b62af492bd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 11.9387, Train Acc: 0.7841\n",
            "Epoch 2, Loss: 9.5216, Train Acc: 0.8441\n",
            "Epoch 3, Loss: 7.6845, Train Acc: 0.8914\n",
            "Epoch 4, Loss: 6.2610, Train Acc: 0.9097\n",
            "Epoch 5, Loss: 5.4448, Train Acc: 0.9249\n",
            "Epoch 6, Loss: 4.4928, Train Acc: 0.9381\n",
            "Epoch 7, Loss: 3.9741, Train Acc: 0.9520\n",
            "Epoch 8, Loss: 3.2279, Train Acc: 0.9628\n",
            "Epoch 9, Loss: 2.3752, Train Acc: 0.9697\n",
            "Epoch 10, Loss: 1.6990, Train Acc: 0.9785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        correct += (outputs.argmax(1) == y_batch).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {correct / len(test_dataset):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgkaFB_yeqVQ",
        "outputId": "b2515fff-0667-4790-d191-8f54424806e2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9144\n"
          ]
        }
      ]
    }
  ]
}